{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ethical-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "global-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "loaded-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider(keyword, since, until, tweets):\n",
    "    # 启动配置\n",
    "    c = twint.Config()\n",
    "    c.Search = keyword\n",
    "    c.Limit = Limit\n",
    "    c.Count = True\n",
    "\n",
    "    # 过滤含连接的tweet\n",
    "    c.Links = \"exclude\"\n",
    "\n",
    "    # 只爬取热门tweet\n",
    "    c.Popular_tweets = True\n",
    "\n",
    "    # 过滤转发\n",
    "    c.Filter_retweets = True\n",
    "\n",
    "    # 统一翻译为英语，不然后面分词时鱼龙混杂\n",
    "    c.Lang = \"en\"\n",
    "    # c.Translate = True\n",
    "    # c.TranslateDest = \"en\"\n",
    "\n",
    "    # 爬取时间段\n",
    "    c.Since = since\n",
    "    c.Until = until\n",
    "    # c.Year = \"2020\"\n",
    "\n",
    "    # 开启存储\n",
    "    c.Store_object = True\n",
    "    c.Store_object_tweets_list = tweets\n",
    "\n",
    "    # 隐藏控制台输出\n",
    "    c.Hide_output = True\n",
    "\n",
    "    # 代理\n",
    "    #c.Proxy_host = '127.0.0.1'\n",
    "    #c.Proxy_port = 7890\n",
    "    #c.Proxy_type = \"socks5\"\n",
    "\n",
    "    # 运行\n",
    "    twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expired-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存数据库\n",
    "def insert_db(sql, value):\n",
    "    conn = pymysql.connect(**config)\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.executemany(sql, value)\n",
    "            conn.commit()\n",
    "            print(\"插入数据库成功\")\n",
    "    except Exception as e:\n",
    "        print(\"插入数据库失败 {}\".format(e))\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latin-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将爬虫和保存数据库操作封装 不采用\n",
    "def crawl_in_db(keyword):\n",
    "    # 爬取结果\n",
    "    tweets = []\n",
    "\n",
    "    # 开始爬取\n",
    "    spider(keyword, tweets) ##这里调用了爬虫函数\n",
    "\n",
    "    # 保存到数据库\n",
    "    sql = \"INSERT INTO `twitter_01` (`date`, `tweet`) VALUES (%s, %s)\"\n",
    "    values = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        values.append((tweet.datestamp, tweet.tweet))\n",
    "\n",
    "    # for i in range(0, len(values), 1000):\n",
    "    #     insert_db(sql, values[i:i+1000])\n",
    "    #     time.sleep(4)\n",
    "    insert_db(sql, values) ##调用了sql存储\n",
    "\n",
    "    print(keyword + \"爬取完毕\")\n",
    "\n",
    "    # 加入延时\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "hydraulic-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将爬虫和保存文件操作封装\n",
    "def crawl_in_file(keyword, since, until):\n",
    "    print(keyword + until +  \"开始爬取\")\n",
    "    # 爬取结果\n",
    "    tweets = []\n",
    "\n",
    "    # 开始爬取b\n",
    "    spider(keyword, since, until, tweets)\n",
    "    # 保存到数据库\n",
    "    \n",
    "    '''\n",
    "    sql安装问题，先注释掉！！！！\n",
    "    sql = \"INSERT INTO `twitter_01` (`date`, `tweet`) VALUES (%s, %s)\"\n",
    "    values = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        values.append((tweet.datestamp, tweet.tweet))\n",
    "\n",
    "    # for i in range(0, len(values), 1000):\n",
    "    #     insert_db(sql, values[i:i+1000])\n",
    "    #     time.sleep(4)\n",
    "    insert_db(sql, values) ##调用了sql存储\n",
    "\n",
    "    '''    \n",
    "    # 文件输出路径\n",
    "    os.chdir(r\"C:/Users/XPS/Desktop/datasave\")\n",
    "    # 获取目标文件夹的路径\n",
    "    filedir = \"C:/Users/XPS/Desktop/datasave\"\n",
    "# 获取当前目录下的文件名列表\n",
    "   ## filename = os.listdir(filedir)\n",
    "# 打开当前目录下的 result1.txt 文件 如果不存在则创建\n",
    "    ##output = open('result1.txt'.format(keyword, keyword + until), 'w',encoding='UTF-8')\n",
    "    ##output = '../data/{}/{}.txt'\n",
    "\n",
    "    # 写入文件\n",
    "    with open('result_confucius_2016.txt'.format(keyword, keyword + until), 'w',encoding='UTF-8') as f:\n",
    "        for tweet_item in tweets:\n",
    "            # 去掉用户名含有关键字\n",
    "            if keyword in tweet_item.username.lower():\n",
    "                # print(tweet_item.username)\n",
    "                continue\n",
    "\n",
    "            # 过滤，为分词做好准备\n",
    "            # 将含有&; # @ $ https的字符串、和较短(1-2位)的字符串删除，\n",
    "            # 删除后会导致头尾空格合在一起，形成多余空格 |\\b\\w{1,2}\\b\n",
    "            first_filter = re.sub(r'\\&\\w*;|#\\w*|@\\w*|\\$\\w*|https?:\\/\\/.*\\/*\\w*|pic.twitter.com\\/*\\w*',\n",
    "                                  '', tweet_item.tweet)\n",
    "            # 将回车和换行替换为空格\n",
    "            # (如果替换为空可能导致两个单词连在一起，但替换为空格则有可能产生多余空格)\n",
    "            second_filter = re.sub(r'\\n+|\\r+|\\t+', ' ', first_filter)\n",
    "            # 去掉多余空格\n",
    "            tweet = re.sub(r'\\s\\s+', ' ', second_filter)\n",
    "\n",
    "            # print(tweet)\n",
    "            f.write(tweet)\n",
    "\n",
    "    print(keyword + until +  \"结束爬取\")\n",
    "    # 加入延时\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prompt-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "##创建路径 需要再修改\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "increasing-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "impressive-reality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confucius2017-01-01 00:00:00开始爬取\n",
      "[+] Finished: Successfully collected 20006 Tweets.\n",
      "confucius2017-01-01 00:00:00结束爬取\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # ####################全局变量#################### #\n",
    "    # 数据库配置\n",
    "    config = {\n",
    "        'host': 'localhost',\n",
    "        'user': 'root',\n",
    "        'password': '0701',\n",
    "        #'port': 3306,\n",
    "        'database': 'Confucius_01',\n",
    "        'charset': 'utf8mb4'\n",
    "    }\n",
    "\n",
    "    # 要限制爬取条数，防止内存压力过大\n",
    "    Limit = 20000\n",
    "    # ############################################### #\n",
    "\n",
    "    # 关键词序列\n",
    "    #keywords = [\"confucian\", \"confucius\", \"confucianism\"]\n",
    "    \n",
    "    ##时间\n",
    "    since = '2016-01-01 00:00:00'\n",
    "    until = '2017-01-01 00:00:00'\n",
    "\n",
    "    # 爬取时间段\n",
    "    times = [\"2018-01-01\", \"2018-01-15\", \"2018-02-01\", \"2018-02-15\", \"2018-03-01\", \"2018-03-15\", \"2018-04-01\",\n",
    "             \"2018-04-15\", \"2018-05-01\", \"2018-05-15\", \"2018-06-01\", \"2018-06-15\", \"2018-07-01\", \"2018-07-15\",\n",
    "             \"2018-08-01\", \"2018-08-15\", \"2018-09-01\", \"2018-09-15\", \"2018-10-01\", \"2018-10-15\", \"2018-11-01\",\n",
    "             \"2018-11-15\", \"2018-12-01\", \"2018-12-15\", \"2019-01-01\", \"2019-01-15\", \"2019-02-01\", \"2019-02-15\",\n",
    "             \"2019-03-01\", \"2019-03-15\", \"2019-04-01\", \"2019-04-15\", \"2019-05-01\", \"2019-05-15\", \"2019-06-01\",\n",
    "             \"2019-06-15\", \"2019-07-01\", \"2019-07-15\", \"2019-08-01\", \"2019-08-15\", \"2019-09-01\", \"2019-09-15\",\n",
    "             \"2019-10-01\", \"2019-10-15\", \"2019-11-01\", \"2019-11-15\", \"2019-12-01\", \"2019-12-15\", \"2020-01-01\",\n",
    "             \"2020-01-15\", \"2020-02-01\", \"2020-02-15\", \"2020-03-01\", \"2020-03-15\", \"2020-04-01\", \"2020-04-15\",\n",
    "             \"2020-05-01\", \"2020-05-15\", \"2020-06-01\", \"2020-06-15\", \"2020-07-01\", \"2020-07-15\", \"2020-08-01\",\n",
    "             \"2020-08-15\", \"2020-09-01\", \"2020-09-15\", \"2020-10-01\", \"2020-10-15\", \"2020-11-01\", \"2020-11-15\",\n",
    "             \"2020-12-01\", \"2020-12-15\", \"2021-01-15\", \"2021-02-01\", \"2021-02-15\", \"2021-03-01\", \"2021-03-15\"]\n",
    "    ##\"2021-04-01\"，\"2021-04-15\", \"2021-05-01\", \"2021-05-15\", \"2021-06-01\", \"2021-06-15\", \"2021-07-01\", \n",
    "    ##\"2021-07-15\", \"2021-08-01\", \"2021-08-15\", \"2021-09-01\", \"2021-09-15\", \"2021-10-01\", \"2021-10-15\"\n",
    "\n",
    "    path = 'C:/Users/XPS/Desktop/datasave' ##这里规定路径\n",
    "    make_dir(path)\n",
    "    \n",
    "    keyword = \"confucius\"\n",
    "    crawl_in_file(keyword, since, until)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    with ThreadPoolExecutor(max_workers=6) as pool:\n",
    "        for keyword in keywords:\n",
    "            path = 'C:/Users/XPS/Desktop/datasave/{}'.format(keyword)\n",
    "            make_dir(path)\n",
    "\n",
    "            for index in range(len(times) - 1):\n",
    "                pool.submit(crawl_in_file(keyword, since, until), keyword, times[index], times[index + 1])\n",
    "    '''    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-incidence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-gospel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-annotation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
